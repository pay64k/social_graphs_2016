{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialgraphs2016/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday November 1st at 23:55. Hand in your IPython notebook file (with extension `.ipynb`) via http://peergrade.io/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part I: Advanced Network Structure\n",
    "\n",
    "We start by looking at the structure of the the philosopher network using the more complicated network measures. If your network has more than one component, just work on the _giant connected component_ (GCC) in the exercises below (in a directed graph use the [_weakly_ connected component](https://networkx.github.io/documentation/networkx-1.9.1/reference/algorithms.component.html)).\n",
    "\n",
    "Not all of the measures we'll be considering below are defined for directed graphs, thus begin by creating an [undirected version](https://networkx.github.io/documentation/networkx-1.9.1/reference/generated/networkx.DiGraph.to_undirected.html) of the philosopher graph, that we can use whenever needed. Only use the undirected graph when explicitly stated in the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1161\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all philosophers from six different branches\n",
    "import io\n",
    "import re\n",
    "\n",
    "branches_of_phi = ['aestheticians', 'epistemologists',\n",
    "                   'ethicists', 'logicians', 'metaphysicians',\n",
    "                   'social_and_political_philosophers']\n",
    "\n",
    "all_phi = []\n",
    "for phi in branches_of_phi:\n",
    "    f = io.open('./wikitext_' + phi + '.txt', 'r', encoding='utf8')\n",
    "    branch_of_phi = re.findall(r'\\[\\[(.*?)\\]\\]', f.read())\n",
    "    all_phi = all_phi + branch_of_phi\n",
    "\n",
    "# Deleting duplicates from prev. list and sort alphabetically\n",
    "philosophers = sorted(set(all_phi))\n",
    "print(len(all_phi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "\n",
    "# Function for retrieving json data from Wikipedia\n",
    "def jsonWiki( name ):\n",
    "    # Parameters for retrieving page from wikipedia\n",
    "    baseurl = 'https://en.wikipedia.org/w/api.php?'\n",
    "    action = \"action=query\"\n",
    "    title = \"titles=\" + name\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataformat = \"format=json\"\n",
    "\n",
    "    # Construct the query\n",
    "    query = \"%s%s&%s&%s&%s\" % (baseurl, action, title, content, dataformat)\n",
    "\n",
    "    # Download json format of wikipedia page\n",
    "    wikiresponse = urllib2.urlopen(query)\n",
    "    wikisource = wikiresponse.read()\n",
    "    wikijson = json.loads(wikisource)\n",
    "    \n",
    "    return wikijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Go through the list 'philosophers' and download their respective pages\n",
    "for i in philosophers:\n",
    "    #Convert philosophers' names from utf8 to string\n",
    "    nameStr = i.encode(\"utf-8\")\n",
    "    \n",
    "    # Whitespace changed to underscore\n",
    "    name_url = re.sub('\\s+', '_', nameStr)\n",
    "    \n",
    "    with io.open('./philosophers_json/' + name_url + '.json', 'w', encoding='utf8') as json_file:\n",
    "        json_file.write(unicode(json.dumps(jsonWiki(name_url), ensure_ascii=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a network called P\n",
    "P = nx.DiGraph()\n",
    "\n",
    "# Go through list of philosophers and add notes to P for every philosopher\n",
    "for i in philosophers:\n",
    "    P.add_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go through list of philosophers and find links\n",
    "for i in philosophers: \n",
    "    # Whitespace changed to underscore\n",
    "    name_url = re.sub('\\s+', '_', i)\n",
    "    \n",
    "    f = io.open('./philosophers_json/' + name_url + '.json', 'r', encoding='utf8')\n",
    "    phi_link = re.findall(r'\\[\\[(.*?)\\]\\]', f.read())\n",
    "    \n",
    "    # Add directed link from philosopher A to philosopher B \n",
    "    for ii in philosophers:\n",
    "        if (ii in phi_link):\n",
    "            P.add_edge(i, ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in P: 1015\n",
      "Number of links in P: 3534\n",
      "Number of nodes in P (undirected): 1015\n",
      "Number of links in P (undirected): 2909\n"
     ]
    }
   ],
   "source": [
    "# Create an undirected graph of network 'P'\n",
    "P_undirected = P.to_undirected()\n",
    "\n",
    "# Number of nodes in 'P'\n",
    "num_of_nodes = P_undirected.number_of_nodes()\n",
    "print('Number of nodes in P: ' + str(num_of_nodes))\n",
    "\n",
    "# Number of links in 'P'\n",
    "num_of_links = P.number_of_edges()\n",
    "print('Number of links in P: ' + str(num_of_links))\n",
    "\n",
    "# Number of nodes in 'P_undirected'\n",
    "num_of_nodes_undirected = P_undirected.number_of_nodes()\n",
    "print('Number of nodes in P (undirected): ' + str(num_of_nodes_undirected))\n",
    "\n",
    "# Number of links in 'P_undirected'\n",
    "num_of_links_undirected = P_undirected.number_of_edges()\n",
    "print('Number of links in P (undirected): ' + str(num_of_links_undirected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the 5 most central philosophers according to [betweenness centrality](https://networkx.github.io/documentation/networkx-1.9.1/reference/generated/networkx.algorithms.centrality.betweenness_centrality.html). What role do you imagine philosophers with high wikipedia graph betweenness centrality play in the history of philosophy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertrand Russell with value of 0.0556127067395\n",
      "Plato with value of 0.0395467362612\n",
      "Aristotle with value of 0.0376407589686\n",
      "David Hume with value of 0.0312091713353\n",
      "Immanuel Kant with value of 0.025797294612\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Betweenness centrality for 'P'\n",
    "between_centrality = nx.betweenness_centrality(P)\n",
    "\n",
    "# Go through 'between_centrality' and add to two lists (names and values)\n",
    "between_centrality_names = []\n",
    "between_centrality_values = []\n",
    "for i in between_centrality:\n",
    "    between_centrality_names.append(i)\n",
    "    between_centrality_values.append(between_centrality[i])\n",
    "    \n",
    "# Combining lists of names and values\n",
    "between_centrality_list = zip(between_centrality_names, between_centrality_values)\n",
    "\n",
    "# Sort by values\n",
    "between_centrality_sorted = sorted(between_centrality_list, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "# Print the top 5\n",
    "for ii in between_centrality_sorted[0:5]:\n",
    "    print(ii[0] + ' with value of ' + str(ii[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Betweened centrality is measuring how central the node is network. Important is that the node with high between centtrality play key role in terms of information transfer in network, under the assumption that information transfer follows the shortest paths. \n",
    " Philosophers with high graph betweenness centrality play in the history of philosophy important role. They have been very influential too other philosophers. They were sort of inspiration to the others. They were precursors of the new field or sub-field of philosophy from which other took inspiration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the 5 most central philosophers according to [eigenvector centrality](https://networkx.github.io/documentation/networkx-1.9.1/reference/generated/networkx.algorithms.centrality.eigenvector_centrality.html). Calculate centrality corresponding to both in- and out-edges (see NetworkX documentation for details). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most central philosophers according to in-edges eigenvector centrality:\n",
      "Aristotle with value of 0.336027671212\n",
      "Bertrand Russell with value of 0.308030228092\n",
      "Plato with value of 0.267924798815\n",
      "Immanuel Kant with value of 0.214854494209\n",
      "Ludwig Wittgenstein with value of 0.194515660319\n",
      "\n",
      "\n",
      "5 most central philosophers according to out-edges eigenvector centrality:\n",
      "Martin Heidegger with value of 0.203269035598\n",
      "Gilles Deleuze with value of 0.174147207718\n",
      "Georg Wilhelm Friedrich Hegel with value of 0.170600021307\n",
      "Henri Bergson with value of 0.166159542536\n",
      "Friedrich Nietzsche with value of 0.164060794333\n"
     ]
    }
   ],
   "source": [
    "# Eigenvector centrality for in-edges\n",
    "eigenvector_centrality_in = nx.eigenvector_centrality(P)\n",
    "\n",
    "# Go through 'eigenvector centrality_in' and add to two lists (names and values)\n",
    "eigenvector_centrality_in_names = []\n",
    "eigenvector_centrality_in_values = []\n",
    "for i in eigenvector_centrality_in:\n",
    "    eigenvector_centrality_in_names.append(i)\n",
    "    eigenvector_centrality_in_values.append(eigenvector_centrality_in[i])\n",
    "    \n",
    "# Combining lists of names and values\n",
    "eigenvector_centrality_in_list = zip(eigenvector_centrality_in_names, eigenvector_centrality_in_values)\n",
    "\n",
    "# Sort by values\n",
    "eigenvector_centrality_in_sorted = sorted(eigenvector_centrality_in_list, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "# Reverse P to find eigenvector centrality for out-edges\n",
    "P_reversed = P.reverse()\n",
    "\n",
    "# Eigenvector centrality for out-edges\n",
    "eigenvector_centrality_out = nx.eigenvector_centrality(P_reversed)\n",
    "\n",
    "# Go through 'eigenvector centrality_out' and add to two lists (names and values)\n",
    "eigenvector_centrality_out_names = []\n",
    "eigenvector_centrality_out_values = []\n",
    "for i in eigenvector_centrality_out:\n",
    "    eigenvector_centrality_out_names.append(i)\n",
    "    eigenvector_centrality_out_values.append(eigenvector_centrality_out[i])\n",
    "    \n",
    "# Combining lists of names and values\n",
    "eigenvector_centrality_out_list = zip(eigenvector_centrality_out_names, eigenvector_centrality_out_values)\n",
    "\n",
    "# Sort by values\n",
    "eigenvector_centrality_out_sorted = sorted(eigenvector_centrality_out_list, key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "print \"5 most central philosophers according to in-edges eigenvector centrality:\"\n",
    "# Print the top 5\n",
    "for ii in eigenvector_centrality_in_sorted[0:5]:\n",
    "    print(ii[0] + ' with value of ' + str(ii[1]))\n",
    "\n",
    "print \"\\n\"\n",
    "print \"5 most central philosophers according to out-edges eigenvector centrality:\"\n",
    "# Print the top 5\n",
    "for ii in eigenvector_centrality_out_sorted[0:5]:\n",
    "    print(ii[0] + ' with value of ' + str(ii[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is eigenvector centrality difference from degree centrality? Compare your results for eigenvector centrality to the results for betweenness centrality - does the difference make sense when you read the philosopher's wikipedia pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEEDS WORK\n",
    "\n",
    "Eigenvector centrality is a measure of how important a node is while Degree centrality is a measure of the amount of links a node has. The difference is that the eigenvector centrality places more importance on what nodes that it is connected while degree centrality is taking into account more the degree of its own nodes and the node with highest degree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is the _undirected version_ of the graph [assortative with respect do degree](https://networkx.github.io/documentation/networkx-1.9.1/reference/generated/networkx.algorithms.assortativity.degree_assortativity_coefficient.html#networkx.algorithms.assortativity.degree_assortativity_coefficient)? (e.g. do high-degree philosophers tend to link to other high-degree philosophers, and low-degree philosophers to other low-degree philosophers?). Provide an interpretation of your answer!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0605791829009\n"
     ]
    }
   ],
   "source": [
    "assortativity = nx.degree_assortativity_coefficient(P_undirected)\n",
    "print assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The philosophers network is a bit dissortative what is result which was expected. High degree philosophers tend to link with other high degree philosophers and low degree philosophers. The different situation is with low degree philosophers who would like to expand their network by linking with more popular philosophers not low degree ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run community detection on the full philosopher network. \n",
    " \n",
    "* Use [the Python Louvain-algorithm implementation](http://perso.crans.org/aynaud/communities/) to find communities in the full philosopher network. Report the value of modularity found by the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aestheticians modularity is 0.0150920006083\n",
      "epistemologists modularity is 0.00949268288302\n",
      "ethicists modularity is 0.112483147274\n",
      "logicians modularity is 0.142679822379\n",
      "metaphysicians modularity is 0.0198245603047\n",
      "social_and_political_philosophers modularity is 0.183412841053\n",
      "\n",
      " Modularity of the Philosopher Network is 0.482985054502\n"
     ]
    }
   ],
   "source": [
    "aestheticians = []\n",
    "f = io.open('./wikitext_aestheticians.txt', 'r', encoding='utf8')\n",
    "aestheticians = re.findall(r'\\[\\[(.*?)\\]\\]', f.read())\n",
    "\n",
    "epistemologists = []\n",
    "f = io.open('./wikitext_epistemologists.txt', 'r', encoding='utf8')\n",
    "epistemologists = re.findall(r'\\[\\[(.*?)\\]\\]', f.read())\n",
    "\n",
    "ethicists = []\n",
    "f = io.open('./wikitext_ethicists.txt', 'r', encoding='utf8')\n",
    "ethicists = re.findall(r'\\[\\[(.*?)\\]\\]', f.read())\n",
    "\n",
    "logicians = []\n",
    "f = io.open('./wikitext_logicians.txt', 'r', encoding='utf8')\n",
    "logicians = re.findall(r'\\[\\[(.*?)\\]\\]', f.read())\n",
    "\n",
    "metaphysicians = []\n",
    "f = io.open('./wikitext_metaphysicians.txt', 'r', encoding='utf8')\n",
    "metaphysicians = re.findall(r'\\[\\[(.*?)\\]\\]', f.read())\n",
    "\n",
    "social_and_political = []\n",
    "f = io.open('./wikitext_social_and_political_philosophers.txt', 'r', encoding='utf8')\n",
    "social_and_political = re.findall(r'\\[\\[(.*?)\\]\\]', f.read())\n",
    "\n",
    "phi_branches_all = aestheticians + epistemologists + ethicists + logicians + metaphysicians + social_and_political\n",
    "\n",
    "phi_branches = [aestheticians, epistemologists, ethicists,\n",
    "                logicians, metaphysicians, social_and_political]\n",
    "\n",
    "# Find philosophers who appear in more than one list and sort them alphabetically\n",
    "phi_duplicates = sorted(set([x for x in all_phi if all_phi.count(x) > 1]))\n",
    "\n",
    "# Remove philosophers who appear more than once\n",
    "phi_no_duplicates = [x for x in phi_branches_all if x not in phi_duplicates]\n",
    "\n",
    "# Find aestheticians who appear in more than one list\n",
    "aestheticians_dup = []\n",
    "for i in aestheticians:\n",
    "    if(i in phi_duplicates):\n",
    "        aestheticians_dup.append(i)\n",
    "        \n",
    "# Remove aestheticians who appear more than once\n",
    "aestheticians_no_dup = [x for x in aestheticians if x not in aestheticians_dup]\n",
    "\n",
    "# Find epistemologists who appear in more than one list\n",
    "epistemologists_dup = []\n",
    "for i in epistemologists:\n",
    "    if(i in phi_duplicates):\n",
    "        epistemologists_dup.append(i)\n",
    "        \n",
    "# Remove epistemologists who appear more than once\n",
    "epistemologists_no_dup = [x for x in epistemologists if x not in epistemologists_dup]\n",
    "\n",
    "# Find ethicists who appear in more than one list\n",
    "ethicists_dup = []\n",
    "for i in ethicists:\n",
    "    if(i in phi_duplicates):\n",
    "        ethicists_dup.append(i)\n",
    "        \n",
    "# Remove ethicists who appear more than once\n",
    "ethicists_no_dup = [x for x in ethicists if x not in ethicists_dup]\n",
    "\n",
    "# Find logicians who appear in more than one list\n",
    "logicians_dup = []\n",
    "for i in logicians:\n",
    "    if(i in phi_duplicates):\n",
    "        logicians_dup.append(i)\n",
    "        \n",
    "# Remove logicians who appear more than once\n",
    "logicians_no_dup = [x for x in logicians if x not in logicians_dup]\n",
    "\n",
    "# Find metaphysicians who appear in more than one list\n",
    "metaphysicians_dup = []\n",
    "for i in metaphysicians:\n",
    "    if(i in phi_duplicates):\n",
    "        metaphysicians_dup.append(i)\n",
    "        \n",
    "# Remove metaphysicians who appear more than once\n",
    "metaphysicians_no_dup = [x for x in metaphysicians if x not in metaphysicians_dup]\n",
    "\n",
    "# Find social_and_political who appear in more than one list\n",
    "social_and_political_dup = []\n",
    "for i in social_and_political:\n",
    "    if(i in phi_duplicates):\n",
    "        social_and_political_dup.append(i)\n",
    "        \n",
    "# Remove social_and_political who appear more than once\n",
    "social_and_political_no_dup = [x for x in social_and_political if x not in social_and_political_dup]\n",
    "philosophers_no_dup_branches = [aestheticians_no_dup, epistemologists_no_dup, ethicists_no_dup,\n",
    "                               logicians_no_dup, metaphysicians_no_dup, social_and_political_no_dup]\n",
    "\n",
    "# Go through list of duplicated philosophers and find most links to branches\n",
    "aestheticians_counter = 0\n",
    "epistemologists_counter = 0\n",
    "ethicists_counter = 0\n",
    "logicians_counter = 0\n",
    "metaphysicians_counter = 0\n",
    "social_and_political_counter = 0\n",
    "phi_counter_list = []\n",
    "for i in phi_duplicates:\n",
    "    # Go through list of neighbours and find the branch the philosopher has the most links to\n",
    "    for ii in P_undirected.neighbors(i):\n",
    "        if(ii in aestheticians):\n",
    "            aestheticians_counter = aestheticians_counter + 1\n",
    "        \n",
    "        if(ii in epistemologists):\n",
    "            epistemologists_counter = epistemologists_counter + 1\n",
    "            \n",
    "        if(ii in ethicists):\n",
    "            ethicists_counter = ethicists_counter + 1\n",
    "            \n",
    "        if(ii in logicians):\n",
    "            logicians_counter = logicians_counter + 1\n",
    "            \n",
    "        if(ii in metaphysicians):\n",
    "            metaphysicians_counter = metaphysicians_counter + 1\n",
    "            \n",
    "        if(ii in social_and_political):\n",
    "            social_and_political_counter = social_and_political_counter + 1\n",
    "\n",
    "    # Add the counter numbers to a list\n",
    "    phi_counter_list.append(aestheticians_counter)\n",
    "    phi_counter_list.append(epistemologists_counter)\n",
    "    phi_counter_list.append(ethicists_counter)\n",
    "    phi_counter_list.append(logicians_counter)\n",
    "    phi_counter_list.append(metaphysicians_counter)\n",
    "    phi_counter_list.append(social_and_political_counter)\n",
    "    \n",
    "    # Find the highest counter in 'phi_counter_list' and its index\n",
    "    max_value = max(phi_counter_list)\n",
    "    max_value_index = phi_counter_list.index(max_value)\n",
    "    \n",
    "    # Add philosopher to the branch he has the most links to\n",
    "    philosophers_no_dup_branches[max_value_index].append(i)\n",
    "    \n",
    "    # Initialize everything for the next philosopher\n",
    "    aestheticians_counter = 0\n",
    "    epistemologists_counter = 0\n",
    "    ethicists_counter = 0\n",
    "    logicians_counter = 0\n",
    "    metaphysicians_counter = 0\n",
    "    social_and_political_counter = 0\n",
    "    phi_counter_list = []\n",
    "\n",
    "# Create subgraph for each branch\n",
    "aestheticians_subgraph = P_undirected.subgraph(aestheticians_no_dup)\n",
    "epistemologists_subgraph = P_undirected.subgraph(epistemologists_no_dup)\n",
    "ethicists_subgraph = P_undirected.subgraph(ethicists_no_dup)\n",
    "logicians_subgraph = P_undirected.subgraph(logicians_no_dup)\n",
    "metaphysicians_subgraph = P_undirected.subgraph(metaphysicians_no_dup)\n",
    "social_and_political_subgraph = P_undirected.subgraph(social_and_political_no_dup)\n",
    "\n",
    "subgraphs_list = [aestheticians_subgraph, epistemologists_subgraph, ethicists_subgraph,\n",
    "                 logicians_subgraph, metaphysicians_subgraph, social_and_political_subgraph]\n",
    "# Modularity: Mc = (Lc/L) - (Kc/2*L)^2\n",
    "# Lc is the total number of links within the community and kc is the total degree of the nodes in this community\n",
    "\n",
    "# Total links in the network\n",
    "L = len(P_undirected.edges())\n",
    "\n",
    "# Go through each subgraph and calculate its modularity\n",
    "total_modularity = 0\n",
    "for i in range(0,6):\n",
    "    Lc = len(subgraphs_list[i].edges())\n",
    "    Kc = len(subgraphs_list[i].nodes())\n",
    "    modularity = (float(Lc)/float(L)) - ((float(Kc)/(2*float(L))))**2\n",
    "    print(branches_of_phi[i] + ' modularity is ' + str(modularity))\n",
    "    total_modularity += modularity\n",
    "\n",
    "print \"\\n Modularity of the Philosopher Network is %s\" %total_modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1015\n",
      "Modularity of the Philosopher Network is 0.478027909969 according to the Louvain algorithm\n"
     ]
    }
   ],
   "source": [
    "import community\n",
    "partition = community.community_louvain.best_partition(P_undirected)\n",
    "modularity = community.community_louvain.modularity(partition, P_undirected)\n",
    "print len(partition)\n",
    "print \"Modularity of the Philosopher Network is %s according to the Louvain algorithm\" % modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Is it higher or lower than what you found above for the branches as communities? What does this comparison reveal about the branches?\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a bit lower than modularity finded for the branches as communities. It means that branches are very good choice for community creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare the communities found by your algorithm with the branches of philosophy (see [Lecture 5](http://nbviewer.jupyter.org/github/suneman/socialgraphs2016/blob/master/lectures/Week5.ipynb) for details on the branches) by creating a matrix **_D_** with dimension (_B_ times _C_), where _B_ is the number of branches and _C_ is the number of communities. We set entry _D_(_i_,_j_) to be the number of nodes that branch _i_ has in common with community _j_. The matrix **_D_** is what we call a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix). Use the confusion matrix to explain how well the communities you've detected correspond to the labeled branches of philosophy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.   0.   0. ...,   0.   0.   0.]\n",
      " [  3.   0.   0. ...,   0.   0.   0.]\n",
      " [ 41.   0.   0. ...,   1.   0.   0.]\n",
      " [ 35.   1.   1. ...,   0.   1.   0.]\n",
      " [ 19.   0.   0. ...,   0.   0.   0.]\n",
      " [  9.   0.   0. ...,   0.   0.   1.]]\n",
      "Confusion matrix shape:\n",
      "(6L, 236L)\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "communities = {}\n",
    "\n",
    "branches=list(map(set,philosophers_no_dup_branches))\n",
    "\n",
    "previous_value = 0\n",
    "not_complete_list = set([])\n",
    "partitions = []\n",
    "for phil in sorted(partition.items(), key=operator.itemgetter(1)):\n",
    "    next_value = phil[1]\n",
    "    if previous_value == next_value:\n",
    "        not_complete_list.add(phil[0])\n",
    "    else:\n",
    "        previous_value = next_value\n",
    "        partitions.append(not_complete_list)\n",
    "        not_complete_list = set([])\n",
    "        not_complete_list.add(phil[0])\n",
    "        \n",
    "# append last value \n",
    "partitions.append(not_complete_list)  \n",
    "\n",
    "C = len(partitions)\n",
    "B = len(branches)\n",
    "D = np.zeros(shape=(B,C))\n",
    "for ii in xrange(0,B):\n",
    "    branch = branches[ii]\n",
    "    for jj in xrange(0,C):\n",
    "        community = partitions[jj]\n",
    "        D[(ii,jj)] = len(branch.intersection(community))\n",
    "        \n",
    "print D\n",
    "shape = D.shape\n",
    "print \"Confusion matrix shape:\" \n",
    "print shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Human navigation paths \n",
    "\n",
    "This exercise works on the wikispeedia dataset. For details on wikispeedia, see [Lecture 8](http://nbviewer.jupyter.org/github/suneman/socialgraphs2016/blob/master/lectures/Week5.ipynb)\n",
    "\n",
    "### IIa: Path lengths\n",
    "\n",
    "The first thing we want to take a look at is path lengths. NetworkX allows us to calculate the shortest path between any pair of articles. We begin by comparing the length of human and shortests paths. \n",
    "\n",
    "* For each _source_/_target_ pair in the list of human navigation paths, calculate the shortest path using NetworkX. Plot the distribution of path lengths. \n",
    "* For each _source_/_target_ pair, calculate the length of the human path. The dataset contains information on people who regret a navigation step and hit the \"back\" button in their web-browser. It's up to you how to incorporate that information in the path. Justify your choice. Plot the distribution of human path lengths. \n",
    "* How much longer are the human paths on average?\n",
    "* Create scatter plot where each point is a _source_/_target_ pair, and you have human path lengths on the $x$-axis and shortests paths on the $y$-axis.\n",
    "* Is there a correlation between human/shortest path-lengths? What is the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIb: Betweenness\n",
    "\n",
    "An interesting definition of centrality is _betweenness centrality_. In a traditional setting, this measure calculates all shortest paths in the network and then each node gets a score according to which fraction of all shortest paths pass through that node.\n",
    "\n",
    "In this part of the assignment, we create our own version of centrality, based on the _source_/_target_ pairs in our dataset. We define a nodes's **navigation centrality** as follows. \n",
    "\n",
    "> *Navigation centrality* of node $i$ is the fraction of all naviagtion paths that pass through $i$. We exclude the source and target from the count. If a node has not been visited by a search, the navigation centrality of that node is defined to be zero.\n",
    "\n",
    "Below, we investigate the relationship between navigation centrality and betweenness centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by calculating the betweenness centrality and navigation centrality of all nodes in the wikispedia dataset.\n",
    "Note that calculating the betweenness centrality can take quite a long time, so you might start it running in a separate notebook while first estimating it based on the existing human path.\n",
    "\n",
    "* First, list the 5 pages with highest navigation centrality.\n",
    "* Second, list the 5 pages with highest betweenness centrality.\n",
    "* Compare the two lists. Explain the differences between the two lists in your own words.\n",
    "* Create a scatterplot of betweenness centrality vs. navigation centrality.\n",
    "* Let's explore the pages that have navigation centrality equal to zero.\n",
    "  * How many pages have zero navigation centrality?\n",
    "  * What is the the page with zero navigation centrality and highest betweenness centrality? Can you explain why no human navigated to this page? Can you explain why the page is central in the actual link network? (For example, you can take a look at the degree of the node).\n",
    "  * Plot the distribution of betweenness centrality for the pages with zero navigation centrality. \n",
    "* Now, let's *throw out all pages with zero navigation centrality* and compare navigation- and betweenness centrality for the remaining pages.\n",
    "  * What is the correlation between betweenness centrality and navigation centrality?\n",
    "  * Comment on the top 5 outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIc: Bringing the text into the picture\n",
    "\n",
    "Now that we have an idea about the differences between how humans and computers search in networks, we are going to dig a little deeper using the page content to test a hypothesis to explain why the human navigation paths are longer. The general idea is that humans (who don't know about the global network structure) tend to jump between pages that have related _content_. For this reason we expect that (on average) human navigation paths have more similar content than the shortest paths in the network (which might take 'surprising' shortcuts via relatively unrelated pages). In short.\n",
    "\n",
    "> **Hypothesis H1**: Human navigation paths have more similar content than network shortest paths.\n",
    "\n",
    "The way we'll test this hypothesis is to first represent each page as a vector using a bag-of-words approach, then we can calculate a distance between pairs of pages using some vector-space difference, and finally we'll characterize each path by its average pair-wise distance. Below, I've set up that process as an exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a TF-IDF vector for each page based on the ascii version of the page texts. \n",
    "\n",
    "Second, write a function that calculates the distance between a pair of vectors. There are many ways to calculate distances between a pair of vectors (try a Google search for `vector space distance measures` if you want to refresh your knowledge on this topic). You're free to choose what you want, but we recommend the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "\n",
    "Now you're ready to get started\n",
    "\n",
    "* Calculate the average similarity for all human navigation paths (the _source_/_target_ pairs from above). Calculate mean/variance of these average similarities.\n",
    "* Calculate the average similarity for all shortest paths between the _source_/_target_ pairs. Calculate mean/variance of these average similarities.\n",
    "* Plot the distributions of average similarities for both human- and shortest paths in a single plot. If everything works well, you should see something similar to the following:\n",
    "![alt text](https://raw.githubusercontent.com/suneman/socialgraphs2016/master/files/path-similarity.png)\n",
    "* Finally, for each source/target pair, compare the human-navigation average similarity with the betweenness based average similarity, testing what fraction of the time, the average similarity is lower in the case of human navigation.\n",
    "* Comment on your findings. Is **H1** true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III\n",
    "\n",
    "Exercise, sentiment over some books from NLPP1e\n",
    "\n",
    "* Download the LabMT wordlist. It's available as supplementary material from [Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752) (Data Set S1). Describe briefly how the list was generated.\n",
    "* Based on the LabMT word list, write a function that calculates sentiment given a list of tokens (the tokens should be lower case, etc). The function should complain if there are no words with sentiment attached.\n",
    "* Calculate a sentiment profile for the novels in NLPP1e chapter 1\\. The sentiment profile has sentiment on the _y_-axis and position in the text on the _x_-axis. Use a [moving average](https://en.wikipedia.org/wiki/Moving_average) to show how the sentiment changes. Create profiles for sliding windows of length 15 words, 50 words, 100 words, 500 words.\n",
    "* Comment on the sentiment profiles. Do they show a similar pattern? What is the effect of changing the size of the sliding window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
