{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Exercise*: Just a couple of examples from the book: Work through the exercises NLPP1e 3.12: 6, 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 6.\n",
    "Describe the class of strings matched by the following regular expressions.\n",
    "    1. [a-zA-Z]+\n",
    "    2. [A-Z][a-z]*\n",
    "    3. p[aeiou]{,2}t\n",
    "    4. \\d+(\\.\\d+)?\n",
    "    5. ([^aeiou][aeiou][^aeiou])*\n",
    "    6. \\w+|[^\\w\\s]+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Matches any word with ASCII characters.\n",
    "2. Matches any word starting with a upper case.\n",
    "3. Matches words that start with *p* and end with *t*, that have 0 to 2 instances of any of these characters *a, e, i, o, u*.\n",
    "4. Matches any positive digit, also those with decimal points.\n",
    "5. ...\n",
    "6. Matches any word character, also unicode charachters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your answers using nltk.re_show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a-zA-Z]+\n",
      "{Hello}, {world}. {paat} {paet} {paaat}. 1, 23, 2.3, -23. Óð{inn}\n",
      "---\n",
      "[A-Z][a-z]*\n",
      "{Hello}, world. paat paet paaat. 1, 23, 2.3, -23. Óðinn\n",
      "---\n",
      "p[aeiou]{,2}t\n",
      "Hello, world. {paat} {paet} paaat. 1, 23, 2.3, -23. Óðinn\n",
      "---\n",
      "\\d+(\\.\\d+)?\n",
      "Hello, world. paat paet paaat. {1}, {23}, {2.3}, -{23}. Óðinn\n",
      "---\n",
      "([^aeiou][aeiou][^aeiou])*\n",
      "{Hello,} {wor}l{}d{}.{} {}p{}a{}a{}t{} {}p{}a{}e{}t{} {}p{}a{}a{}a{}t{}.{} {}1{},{} {}2{}3{},{} {}2{}.{}3{},{} {}-{}2{}3{}.{} {}�{}�{}�{�in}n{}\n",
      "---\n",
      "\\w+|[^\\w\\s]+\n",
      "{Hello}{,} {world}{.} {paat} {paet} {paaat}{.} {1}{,} {23}{,} {2}{.}{3}{,} {-}{23}{.} {Óð}{inn}\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "string = 'Hello, world. paat paet paaat. 1, 23, 2.3, -23. Óðinn'\n",
    "reg_exp = ['[a-zA-Z]+', '[A-Z][a-z]*', 'p[aeiou]{,2}t',\n",
    "           '\\d+(\\.\\d+)?', '([^aeiou][aeiou][^aeiou])*',\n",
    "           '\\w+|[^\\w\\s]+']\n",
    "\n",
    "for i in reg_exp:\n",
    "    print i\n",
    "    nltk.re_show(i, string)\n",
    "    print '---'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30.\n",
    "Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENNIS: Listen, strange women lying in ponds distributing swords\n",
      "is no basis for a system of government.  Supreme executive power derives from\n",
      "a mandate from the masses, not from some farcical aquatic ceremony.\n"
     ]
    }
   ],
   "source": [
    "# Raw text\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "print(raw)\n",
    "\n",
    "# Tokenize the raw text\n",
    "tokens = nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENNI\n",
      ":\n",
      "Listen\n",
      ",\n",
      "strang\n",
      "women\n",
      "lie\n",
      "in\n",
      "pond\n",
      "distribut\n",
      "sword\n",
      "is\n",
      "no\n",
      "basi\n",
      "for\n",
      "a\n",
      "system\n",
      "of\n",
      "govern\n",
      ".\n",
      "Suprem\n",
      "execut\n",
      "power\n",
      "deriv\n",
      "from\n",
      "a\n",
      "mandat\n",
      "from\n",
      "the\n",
      "mass\n",
      ",\n",
      "not\n",
      "from\n",
      "some\n",
      "farcic\n",
      "aquat\n",
      "ceremoni\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Porter Stemmer\n",
    "porter = nltk.PorterStemmer()\n",
    "for t in tokens:\n",
    "    print(porter.stem(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "den\n",
      ":\n",
      "list\n",
      ",\n",
      "strange\n",
      "wom\n",
      "lying\n",
      "in\n",
      "pond\n",
      "distribut\n",
      "sword\n",
      "is\n",
      "no\n",
      "bas\n",
      "for\n",
      "a\n",
      "system\n",
      "of\n",
      "govern\n",
      ".\n",
      "suprem\n",
      "execut\n",
      "pow\n",
      "der\n",
      "from\n",
      "a\n",
      "mand\n",
      "from\n",
      "the\n",
      "mass\n",
      ",\n",
      "not\n",
      "from\n",
      "som\n",
      "farc\n",
      "aqu\n",
      "ceremony\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Lancaster Stemmer\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "for t in tokens:\n",
    "    print(lancaster.stem(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercises: TF-IDF and the branches of philosophy.\n",
    "\n",
    "Setup. We want to start from a clean version of the philosopher pages with as little wiki-markup as possible. We needed it earlier to get the links, etc, but now we want a readable version. We can get a fairly nice version directly from the wikipedia API, simply call prop=extracts&exlimit=max&explaintext instead of prop=revisions as we did earlier. This will make the API return the text without links and other markup.\n",
    "* Use this method to retrive a nice copy of all philosopher's text. You can, of course, also clean the existing pages using regular expressions, if you like (but that's probably more work).\n",
    "\n",
    "The exercise.\n",
    "* First, check out [the wikipedia page for TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). Explain in your own words the point of TF-IDF.\n",
    "    * What does TF stand for?\n",
    "    * What does IDF stand for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since we want to find out which words are important for each branch, so we're going to create six large documents, one per branch of philosophy. Tokenize the pages, and combine the tokens into one long list per branch. Remember the bullets below for success.\n",
    "    * If you dont' know what tokenization means, go back and read Chapter 3 again. This advice is valid for every cleaning step below.\n",
    "    * Exclude philosopher names (since we're interested in the words, not the names).\n",
    "    * Exclude punctuation.\n",
    "    * Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).\n",
    "    * Exclude numbers (since they're difficult to interpret in the word cloud).\n",
    "    * Set everything to lower case.\n",
    "    * Note that none of the above has to be perfect. It might not be easy to remove all philosopher names. And there's some room for improvisation. You can try using stemming. In my own first run the results didn't look so nice, because some pages are very detailed and repeat certain words again and again and again, whereas other pages are very short. For that reason, I decided to use the unique set of words from each page rather than each word in proportion to how it's actually used on that page. Choices like that are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
